import pandas as pd
import json
from pathlib import Path
from typing import Optional
from langchain_core.tools import Tool


def update_slow_sql_csv_result(input_json: str) -> str:
    """
    æ›´æ–°æ…¢SQLå®¡æ‰¹CSVæ–‡ä»¶ï¼Œå°†åˆ†æç»“æœå†™å…¥å¯¹åº”è¡Œçš„ä¸‹ä¸€åˆ—
    ä¸ä¿®æ”¹åŸæ–‡ä»¶ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªæ–°çš„ä¿®æ”¹åçš„CSVæ–‡ä»¶
    
    Args:
        input_json: JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        {
            "sql_hash": "æ…¢SQLçš„hashå€¼",
            "result_type": "ddl" | "mr" | "ai_analysis",  # ç»“æœç±»å‹
            "result_content": "DDLè¯­å¥ | MRé“¾æ¥ | AIåˆ†æè¾“å‡º",
            "ai_decision": "å¯é€‰ï¼šAIå†³ç­–çš„å®Œæ•´JSON" # ä»…å½“result_typeä¸º"mr"æ—¶éœ€è¦
        }
    
    Returns:
        str: æ“ä½œç»“æœæ¶ˆæ¯
    """
    try:
        # è§£æè¾“å…¥å‚æ•°
        input_data = json.loads(input_json)
        sql_hash = input_data["sql_hash"]
        result_type = input_data["result_type"]  # "ddl", "mr", æˆ– "ai_analysis"
        result_content = input_data["result_content"]
        ai_decision = input_data.get("ai_decision")  # å¯é€‰çš„AIå†³ç­–JSON
        
        # åŸCSVæ–‡ä»¶è·¯å¾„
        original_csv_path = Path("/Users/moka/PycharmProjects/SentryAgent/å¤–éƒ¨æ–‡ä»¶/å®¡æ‰¹æ…¢SQL.csv")
        # ä¿®æ”¹åçš„CSVæ–‡ä»¶è·¯å¾„
        updated_csv_path = Path("/Users/moka/PycharmProjects/SentryAgent/å¤–éƒ¨æ–‡ä»¶/å®¡æ‰¹æ…¢SQL_å·²åˆ†æ.csv")
        
        # ç¡®å®šè¦è¯»å–çš„CSVæ–‡ä»¶ï¼ˆä¼˜å…ˆè¯»å–å·²æœ‰çš„åˆ†æç»“æœæ–‡ä»¶ï¼‰
        if updated_csv_path.exists():
            csv_file_path = updated_csv_path
            print(f"[ä¿¡æ¯] è¯»å–å·²æœ‰çš„åˆ†æç»“æœæ–‡ä»¶: {csv_file_path}")
        elif original_csv_path.exists():
            csv_file_path = original_csv_path
            print(f"[ä¿¡æ¯] è¯»å–åŸå§‹CSVæ–‡ä»¶: {csv_file_path}")
        else:
            return f"é”™è¯¯ï¼šCSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{original_csv_path}"
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
        
        # æŸ¥æ‰¾å¯¹åº”çš„SQL hashè¡Œ
        hash_column = 'sql_hash'
        if hash_column not in df.columns:
            return f"é”™è¯¯ï¼šCSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ°sql_hashåˆ—"
        
        # æ‰¾åˆ°åŒ¹é…çš„è¡Œ
        matching_rows = df[df[hash_column] == sql_hash]
        
        if matching_rows.empty:
            return f"é”™è¯¯ï¼šæœªæ‰¾åˆ°hashä¸º {sql_hash} çš„è®°å½•"
        
        if len(matching_rows) > 1:
            return f"è­¦å‘Šï¼šæ‰¾åˆ°å¤šæ¡hashä¸º {sql_hash} çš„è®°å½•ï¼Œå°†æ›´æ–°ç¬¬ä¸€æ¡"
        
        # è·å–è¡Œç´¢å¼•
        row_index = matching_rows.index[0]
        
        # ç¡®å®šè¦æ›´æ–°çš„åˆ—å¹¶æ·»åŠ åˆ—æ ‡é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if result_type == "ddl":
            column_index = 5
            column_name = "DDLå»ºè®®"
            result_label = "DDLå»ºè®®"
        elif result_type == "mr":
            column_index = 6
            column_name = "MRé“¾æ¥"
            result_label = "MRé“¾æ¥"
        elif result_type == "ai_analysis":
            column_index = 7
            column_name = "AIåˆ†æ"
            result_label = "AIåˆ†æ"
        else:
            return f"é”™è¯¯ï¼šä¸æ”¯æŒçš„ç»“æœç±»å‹ï¼š{result_type}ï¼Œæ”¯æŒ'ddl'ã€'mr'æˆ–'ai_analysis'"
        
        # ç¡®ä¿CSVæœ‰è¶³å¤Ÿçš„åˆ—ï¼Œå¹¶è®¾ç½®åˆ—å
        while len(df.columns) <= column_index:
            new_col_name = f"åˆ†æç»“æœ_{len(df.columns) - 4}"  # ä»ç¬¬5åˆ—å¼€å§‹æ˜¯åˆ†æç»“æœ
            df[new_col_name] = ''
        
        # è®¾ç½®ç‰¹å®šåˆ—çš„åç§°
        if column_index == 5:
            df.rename(columns={df.columns[5]: "DDLå»ºè®®"}, inplace=True)
        elif column_index == 6:
            df.rename(columns={df.columns[6]: "MRé“¾æ¥"}, inplace=True)
        elif column_index == 7:
            df.rename(columns={df.columns[7]: "AIåˆ†æ"}, inplace=True)
        
        # æ›´æ–°æŒ‡å®šè¡Œå’Œåˆ—ï¼Œç¡®ä¿åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®
        df.iloc[row_index, column_index] = str(result_content)
        
        # å¦‚æœæ˜¯MRç±»å‹ä¸”æœ‰AIå†³ç­–æ•°æ®ï¼ŒåŒæ—¶ä¿å­˜åˆ°AIåˆ†æåˆ—
        if result_type == "mr" and ai_decision:
            # ç¡®ä¿AIåˆ†æåˆ—å­˜åœ¨
            while len(df.columns) <= 7:
                new_col_name = f"åˆ†æç»“æœ_{len(df.columns) - 4}"
                df[new_col_name] = ''
            if len(df.columns) > 7:
                df.rename(columns={df.columns[7]: "AIåˆ†æ"}, inplace=True)
            
            # æ ¼å¼åŒ–AIå†³ç­–æ•°æ®ï¼ˆå‹ç¼©æ ¼å¼ï¼Œæ— æ¢è¡Œç¬¦ï¼‰
            ai_analysis_content = json.dumps(ai_decision, ensure_ascii=False, separators=(',', ':'))
            df.iloc[row_index, 7] = ai_analysis_content
        
        # ä¿å­˜åˆ°ä¿®æ”¹åçš„CSVæ–‡ä»¶
        df.to_csv(updated_csv_path, index=False, encoding='utf-8-sig')
        
        return f"æˆåŠŸï¼šå·²å°†{result_label}æ›´æ–°åˆ°åˆ†æç»“æœæ–‡ä»¶ï¼Œhash={sql_hash}ï¼Œå†…å®¹={result_content}ï¼Œæ–‡ä»¶è·¯å¾„ï¼š{updated_csv_path}"
        
    except json.JSONDecodeError as e:
        return f"é”™è¯¯ï¼šJSONè§£æå¤±è´¥ï¼š{e}"
    except Exception as e:
        return f"é”™è¯¯ï¼šæ›´æ–°CSVæ–‡ä»¶å¤±è´¥ï¼š{e}"


def read_slow_sql_by_hash(sql_hash: str) -> Optional[dict]:
    """
    æ ¹æ®hashè¯»å–æ…¢SQLä¿¡æ¯ï¼Œä¼˜å…ˆä»åˆ†æç»“æœæ–‡ä»¶è¯»å–
    
    Args:
        sql_hash: SQLçš„hashå€¼
        
    Returns:
        dict: åŒ…å«SQLä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    try:
        # åŸCSVæ–‡ä»¶è·¯å¾„
        original_csv_path = Path("/Users/moka/PycharmProjects/SentryAgent/å¤–éƒ¨æ–‡ä»¶/å®¡æ‰¹æ…¢SQL.csv")
        # ä¿®æ”¹åçš„CSVæ–‡ä»¶è·¯å¾„
        updated_csv_path = Path("/Users/moka/PycharmProjects/SentryAgent/å¤–éƒ¨æ–‡ä»¶/å®¡æ‰¹æ…¢SQL_å·²åˆ†æ.csv")
        
        # ç¡®å®šè¦è¯»å–çš„CSVæ–‡ä»¶ï¼ˆä¼˜å…ˆè¯»å–å·²æœ‰çš„åˆ†æç»“æœæ–‡ä»¶ï¼‰
        if updated_csv_path.exists():
            csv_file_path = updated_csv_path
            print(f"[ä¿¡æ¯] ä»åˆ†æç»“æœæ–‡ä»¶è¯»å–: {csv_file_path}")
        elif original_csv_path.exists():
            csv_file_path = original_csv_path
            print(f"[ä¿¡æ¯] ä»åŸå§‹æ–‡ä»¶è¯»å–: {csv_file_path}")
        else:
            print(f"é”™è¯¯ï¼šCSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{original_csv_path}")
            return None
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
        
        # æŸ¥æ‰¾å¯¹åº”çš„SQL hashè¡Œ
        matching_rows = df[df['sql_hash'] == sql_hash]
        
        if matching_rows.empty:
            print(f"æœªæ‰¾åˆ°hashä¸º {sql_hash} çš„è®°å½•")
            return None
            
        # è·å–ç¬¬ä¸€è¡ŒåŒ¹é…è®°å½•
        row = matching_rows.iloc[0]
        
        result = {
            "sql_hash": row['sql_hash'],
            "create_time": row['create_time'],
            "count": row['count'],
            "sql_text": row['sql_text']
        }
        
        # å¦‚æœæ˜¯ä»åˆ†æç»“æœæ–‡ä»¶è¯»å–ï¼Œè¿˜åŒ…å«åˆ†æç»“æœ
        if csv_file_path == updated_csv_path:
            if 'DDLå»ºè®®' in df.columns and pd.notna(row.get('DDLå»ºè®®')):
                result['ddl_suggestion'] = row['DDLå»ºè®®']
            if 'MRé“¾æ¥' in df.columns and pd.notna(row.get('MRé“¾æ¥')):
                result['mr_link'] = row['MRé“¾æ¥']
            if 'AIåˆ†æ' in df.columns and pd.notna(row.get('AIåˆ†æ')):
                result['ai_analysis'] = row['AIåˆ†æ']
        
        return result
        
    except Exception as e:
        print(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ï¼š{e}")
        return None


# åˆ›å»ºLangChainå·¥å…·
updateSlowSqlCsvResultTool = Tool(
    name="updateSlowSqlCsvResultTool",
    func=update_slow_sql_csv_result,
    description="æ›´æ–°æ…¢SQLå®¡æ‰¹CSVæ–‡ä»¶ï¼Œå°†DDLå»ºè®®æˆ–MRé“¾æ¥å†™å…¥å¯¹åº”hashè¡Œçš„ç»“æœåˆ—ä¸­"
)

readSlowSqlByHashTool = Tool(
    name="readSlowSqlByHashTool", 
    func=lambda hash: json.dumps(read_slow_sql_by_hash(hash), ensure_ascii=False) if read_slow_sql_by_hash(hash) else "æœªæ‰¾åˆ°å¯¹åº”è®°å½•",
    description="æ ¹æ®hashä»æ…¢SQLå®¡æ‰¹CSVæ–‡ä»¶ä¸­è¯»å–SQLä¿¡æ¯"
)

if __name__ == "__main__":
    print("=== æµ‹è¯•ä¼˜åŒ–åçš„CSVæ›´æ–°é€»è¾‘ï¼ˆåŒ…å«AIåˆ†æåˆ—ï¼‰===")
    
    # æµ‹è¯•è¯»å–åŠŸèƒ½
    print("1. æµ‹è¯•è¯»å–æ…¢SQLä¿¡æ¯")
    sql_info = read_slow_sql_by_hash("299560ec4687fa45916095da5f793c06")
    print("è¯»å–ç»“æœï¼š", json.dumps(sql_info, ensure_ascii=False, indent=2) if sql_info else "æœªæ‰¾åˆ°")
    
    # æµ‹è¯•æ›´æ–°åŠŸèƒ½ - DDL
    print("\n2. æµ‹è¯•æ›´æ–°DDLç»“æœ")
    ddl_update = {
        "sql_hash": "test_hash_ddl_001",
        "result_type": "ddl",
        "result_content": "CREATE INDEX `idx_proc_inst_id` ON `act_hi_taskinst` (`PROC_INST_ID_`);"
    }
    result = update_slow_sql_csv_result(json.dumps(ddl_update, ensure_ascii=False))
    print("DDLæ›´æ–°ç»“æœï¼š", result)
    
    # æµ‹è¯•æ›´æ–°åŠŸèƒ½ - å•ç‹¬çš„AIåˆ†æ
    print("\n3. æµ‹è¯•å•ç‹¬ä¿å­˜AIåˆ†æç»“æœ")
    ai_analysis_update = {
        "sql_hash": "test_hash_ai_001",
        "result_type": "ai_analysis",
        "result_content": json.dumps({
            "decision_type": "table_design",
            "reason": "æµ‹è¯•åŸå› ï¼šè¯¥è¡¨ç¼ºå°‘ç´¢å¼•",
            "fix_suggest": "æµ‹è¯•å»ºè®®ï¼šåˆ›å»ºç´¢å¼•æå‡æŸ¥è¯¢æ€§èƒ½",
            "fix_server": "test-service"
        }, ensure_ascii=False, separators=(',', ':'))
    }
    result = update_slow_sql_csv_result(json.dumps(ai_analysis_update, ensure_ascii=False))
    print("AIåˆ†ææ›´æ–°ç»“æœï¼š", result)
    
    # æµ‹è¯•æ›´æ–°åŠŸèƒ½ - MRå¸¦AIå†³ç­–
    print("\n4. æµ‹è¯•æ›´æ–°MRç»“æœï¼ˆåŒæ—¶ä¿å­˜AIå†³ç­–ï¼‰")  
    mr_with_ai_update = {
        "sql_hash": "test_hash_mr_001",
        "result_type": "mr", 
        "result_content": "https://gitlab.mokahr.com/hcm-developer/workflow-process/-/merge_requests/123",
        "ai_decision": {
            "decision_type": "business_logic",
            "reason": "è¯¥SQLæ¥è‡ªæ‰¹é‡æŸ¥è¯¢æ–¹æ³•ï¼Œä¼ å…¥å‚æ•°è¿‡å¤šå¯¼è‡´INæŸ¥è¯¢æ€§èƒ½ä¸‹é™",
            "fix_suggest": "å»ºè®®åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š200ä¸ªå‚æ•°",
            "fix_server": "hcm-workflow-process-platform"
        }
    }
    result = update_slow_sql_csv_result(json.dumps(mr_with_ai_update, ensure_ascii=False))
    print("MR+AIå†³ç­–æ›´æ–°ç»“æœï¼š", result)
    
    # éªŒè¯è¯»å–åŒ…å«æ‰€æœ‰åˆ†æç»“æœ
    print("\n5. éªŒè¯è¯»å–åŒ…å«å®Œæ•´åˆ†æç»“æœ")
    test_cases = ["test_hash_ddl_001", "test_hash_ai_001", "test_hash_mr_001"]
    for test_hash in test_cases:
        sql_info_with_analysis = read_slow_sql_by_hash(test_hash)
        print(f"\næµ‹è¯•hash {test_hash}:")
        if sql_info_with_analysis:
            for key, value in sql_info_with_analysis.items():
                if key == 'ai_analysis' and value:
                    # AIåˆ†æå†…å®¹è¾ƒé•¿ï¼Œæ ¼å¼åŒ–æ˜¾ç¤º
                    try:
                        ai_data = json.loads(value)
                        print(f"  {key}: {json.dumps(ai_data, ensure_ascii=False, indent=4)}")
                    except:
                        print(f"  {key}: {value}")
                else:
                    print(f"  {key}: {value}")
        else:
            print("  æœªæ‰¾åˆ°è®°å½•")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ åŸæ–‡ä»¶ä¿æŒä¸å˜ï¼šå¤–éƒ¨æ–‡ä»¶/å®¡æ‰¹æ…¢SQL.csv") 
    print("ğŸ“ åˆ†æç»“æœä¿å­˜è‡³ï¼šå¤–éƒ¨æ–‡ä»¶/å®¡æ‰¹æ…¢SQL_å·²åˆ†æ.csv")
    print("ğŸ“Š æ–°å¢åŠŸèƒ½ï¼š")
    print("  - DDLå»ºè®®åˆ—ï¼ˆç¬¬5åˆ—ï¼‰")
    print("  - MRé“¾æ¥åˆ—ï¼ˆç¬¬6åˆ—ï¼‰") 
    print("  - AIåˆ†æåˆ—ï¼ˆç¬¬7åˆ—ï¼‰- å‹ç¼©JSONæ ¼å¼")
    print("  - MRæäº¤æ—¶è‡ªåŠ¨ä¿å­˜AIå†³ç­–æ•°æ®")